\chapter{Method and Datasets}
\label{sec:method}
In this chapter, the specific neural network used to denoise \acrshort{ct} images will be introduced, some image comparison metrics will be given alongside a brief discussion of them, the used datasets will be described, and the method used to compile a given dataset into a suitable format for the neural network will be explained. 


\section{TomoGAN}
\label{sec:method:tomogan}
The denoising neural network used in this thesis is called TomoGAN \cite{liu2020tomogan}. It is a \acrshort{gan} where the generator is based on the U-net model \cite{unet}, and the input to the \acrshort{gan} is not some random noise but rather some noisy image. There are some key differences in the generator from the U-net model, namely \cite{liu2020tomogan}:
\begin{itemize}
    \item There are three (instead of four) down- and up sampling layers.
    \item All convolutions have zero-padding to keep image dimensions unchanged during convolutions.
    \item The generator input is a stack of $d$ adjacent slices where eight $1\times1$ convolutions are applied. 
\end{itemize}
In the down sampling part of the generator, three sets of two $3\times3$ convolutional layers with ReLU activation functions extract feature maps. Between each set of convolutional layers there is a $2\times2$ max pooling layer, for a total down sampling of $8$ leading to a feature map of size $1/8$ of the original image in either dimension (e.g. images of dimensions $1024\times1024$ will result in a feature map of dimensions $128\times128$). The number of channels in a convolutional layer changes throughout the network begining with $8$ channels, and the feature map after the down sampling process has $128$ channels. 

The up sampling part of the generator is symmetric to the down sampling part, containing three sets of two $3\times3$ convolutional layers. The max pooling layers are replaced by bi-linear interpolation layers that instead of reducing the size of the feature map, increase it. Again, all the convolutional kernels have kernels of size $3\times3$ and use ReLU activation functions. At each of the three sets of convolutional layers, the feature maps from the corresponding set in the down sampling section are concatenated to the feature maps outputted from the previous set of convolutional layers\footnote{These skip-connection, as they are often called, are what make this structure a U-net style network and not simply an encoder-decoder network \cite{unet}. }. Finally, a $1\times1$ convolutional layer with a ReLU activation function followed by a $1\times1$ convolutional layer with a linear activation function is used to combine all the channels into a final output image. 

\missingfigure{TODO: TomoGAN network structure. }

The discriminator network used to help train TomoGAN consists of six 2D $3\times3$ convolutional layers with leaky ReLU activation functions, as well as two fully connected layers. \todo[]{Write about Wasserstein distance and its use as loss function? }

No changes have been made to the structure of the TomoGAN network in this thesis, however some changes have been made to the loss function used to train the network. The original TomoGAN network, as given in \cite{liu2020tomogan}, was trained using a loss function comprising of an \acrshort{mse} loss, a feature based (VGG) loss, and an adversarial loss, given as 
\begin{equation}
    L_{\text{Total}} = \lambda_{\text{MSE}}L_{\text{MSE}} + \lambda_{\text{VGG}}L_{\text{VGG}} + \lambda_{\text{Adv}}L_{\text{Adv}}.
\end{equation}
Based on articles citing the log-cosh loss function as a good candidate for image processing \cite{7797130,chen2019log} it was added as an additional term to the total loss, giving the overall loss function
\begin{equation}
    L_{\text{Total}} = \lambda_{\text{MSE}}L_{\text{MSE}} + \lambda_{\text{Log-cosh}}L_{\text{Log-cosh}} + \lambda_{\text{VGG}}L_{\text{VGG}} + \lambda_{\text{Adv}}L_{\text{Adv}}.
\end{equation}
The weights for the different loss functions were mostly unchanged, with only $\lambda_\text{MSE}$ being slightly reduced to account for the fact that $L_\text{Log-cosh}$ covers a similar role (as they have similar activations). 

\section{Image Comparison Metrics}
\label{sec:method:metrics}
To properly quantify the performance of the denoising method, some metrics must be defined. While there are some standards to doing this, not all methods perform equally well. 

\subsection{Mean Squared Error}
\label{sec:method:metrics:mse}
The most commonly used full-reference image quality metric is the \acrfull{mse}, as defined (as a loss function) in \cref{eq:lossmse}. While it provides some sense of similarity and is very easy to calculate and physically understand, a low (i.e. good) \acrshort{mse} does not necessarily correspond to a high degree of visual similarity and it is therefore not a good metric to compare visual similarities in images \cite{413502,477498}. Likewise, the \acrfull{psnr}, which is another very commonly used metric, does not correspond to visual similarity. The \acrshort{psnr} can be defined from the \acrshort{mse}, and is given as \cite{477498}
\begin{equation}
    \label{eq:psnr}
    \text{PSNR} = 10 \cdot \log_{10} \left( \frac{\text{MAX}_Y}{MSE^2} \right),
\end{equation}
where $\text{MAX}_Y$ is the maximal pixel value possible in the image (e.g. for an 8-bit image it is $2^8-1=255$), and $MSE$ is the \acrshort{mse} of the image. 

\subsection{Structural Similarity Index Measure}
The \acrfull{ssim} is a metric to measure similarity between images \cite{ssim}. It is a full-reference image quality assessment, meaning it compares a complete reference high-quality image to a full low-quality image\footnote{Other types of image quality assessments are either no-reference where there is no high-quality reference image to compare to, or reduced-reference where there is some partial information from a reference image to compare to (e.g. a set of extracted features)\cite{ssim}. }. 


The \acrshort{ssim} takes into account three metrics of the image: luminance, contrast, and structure. They are combined, giving the definition of \acrshort{ssim} as \cite{ssim}
\begin{equation}
    \label{eq:ssim}
    \text{SSIM}\left(x,y\right) = \frac{\left( 2\mu_x \mu_y + C_1 \right) \left( 2\sigma_{xy} + C_2 \right)}{\left( \mu_x^2 + \mu_y^2 + C_1 \right) \left( \sigma_x^2 + \sigma_y^2 + C_2 \right)},
\end{equation}
where $x$ and $y$ are the two images to compare, $\mu$ is the mean pixel value of an image, $\sigma$ is the standard deviation of the pixel values of an image, and $C_{\{1,2\}}$ are regularization constants. It is symmetric (i.e. $\text{SSIM}\left(x,y\right) = \text{SSIM}\left(y,x\right)$), bounded (i.e. $\text{SSIM}\left(x,y\right) \leq 1$), and has a unique maximum (i.e. $\text{SSIM}\left(x,y\right) = 1$ if and only if $x = y$) \cite{ssim}. A more robust analysis of the mathematical properties of the \acrshort{ssim} was performed in \cite{6059504}.

\missingfigure{TODO: Maybe add figure showing difference in MSE and SSIM performance, similar to fig. 2 in \cite{ssim}. }

\section{Datasets}
\label{sec:method:datasets}
A selection of different datasets have been used to train and test the TomoGAN network. Some of these have been collected from TomoBank \cite{TomoBank}, one is collected in-house, and one is \todo[]{write about ASM\_ID16B dataset}. This section will give a description of each used dataset. 

\subsection{TomoBank}
TomoBank is an X-ray tomography data bank providing experimental and simulated datasets with the aim to foster collaboration among computational scientists, beamline scientists, and experimentalists \cite{TomoBank}. It provides several types of datasets imaging different samples as well as simulated phantoms. Some of these datasets have been used in this thesis.

The tomo\_00058 dataset is a dataset imaging borosilicate glass spheres encased in a polypropylene matrix \cite{datasetglassspheres}. It contains a $20\%$ concentration of glass spheres. The technical information of the dataset can be seen in \cref{tab:tomo00058}, and an overview of how many projections are used in different subsamplings to simulate missing wedge noise can be seen in \cref{tab:projectionsubsampling}. 

\begin{table}[htbp]
    \centering
    \caption[Dataset information tomo\_00058]{Overview of technical information of the tomo\_00058 dataset. A more complete overview is available from \cite{datasetglassspheres}. }
    \label{tab:tomo00058}
    \begin{tabular}{ll}
    \hline
    Instrument & APS 2-BM-A fast tomo \\
    Scan Range & 180 degree \\
    Number of Projections & 1500 \\
    Energy & \SI{27.4}{\kilo \electronvolt}\\
    Exposure Time & \SI{1}{\milli \second}\\
    Pixel Size & \SI{0.65}{\micro \meter} \\
    Detector Dimension x & 2560 \\
    Detector Dimension y & 2160 \\
    \hline
    \end{tabular}
\end{table}

\begin{table}[htbp]
    \centering
    \caption[Projection subsampling overview tomo\_00058]{Overview of the number of projections for different subsampling factors of tomo\_00058. }
    \label{tab:projectionsubsampling}
    \begin{tabular}{ll}
    \hline
    Subsampling Factor & Projections \\
    \hhline{==}
    $1$ & $1500$ \\
    $8$ & $187$ \\
    $16$ & $93$ \\
    $32$ & $46$ \\
    $48$ & $31$ \\
    \hline
    \end{tabular}
\end{table}

Two datasets of shale from TomoBank were also used, namely tomo\_00001 and tomo\_00002 \cite{datasetshale}. These datasets are two shale samples collected from the North Sea and from the Upper Barnett Formation in Texas. They are both imaged at the Advanced Photon Source (APS) of Argonne National Laboratory, USA. These datasets are only used to train a network for denoising of another shale sample, the ASM\_ID16B shale sample. 

All datasets used from TomoBank were reconstructed using \acrshort{fbp} from the TomoPy library \cite{tomopy}. 


\todo[inline]{ASM\_ID16B\_phaseContrast\_Shale\_2018, not in-house}
\subsection{In-house}
An in-house captured dataset imaging a sample of soda lime glass spheres in a capillary tube containing a potassium iodide (KI) doped water solution has been used. The soda lime glass spheres have diameters in the \SIrange{1250}{1650}{\micro \meter} range, the capillary tube has an inner diameter of \SI{2.5}{\milli \meter} and an outer diameter of \SI{4.0}{\milli \meter}, and the potassium iodide doped water has a concentration of \SI{0.5}{\molar} KI. The sample was imaged twice: one slow \acrlong{hq} imaging, and one fast \acrlong{lq} imaging. The technical information of the two imagings of the in-house dataset can be seen in \cref{tab:inhousehq,tab:inhouselq}.

For the sake of simplicity, the two datasets will be refered to as the \acrfull{ihhq} and the \acrfull{ihlq} datasets. 

Because the used \acrshort{ct} instrument has a conical beam, the \acrshort{fdk} reconstruction algorithm was used to reconstruct these datasets. A \acrshort{piccs} reconstruction of the \acrshort{ihlq} dataset is also included as an alternative denoising method for comparison. 

\begin{table}[htbp]
    \centering
    \caption[Technical information of the IHHQ dataset]{Overview of technical information of the \acrlong{hq} imaging of the in-house dataset. This dataset is refered to as the \acrshort{ihhq} dataset. }
    \label{tab:inhousehq}
    \begin{tabular}{ll}
    \hline
    Instrument & Nikon XT H 225 ST \\
    Number of Projections & 1570 \\
    Voltage & \SI{170}{\kilo \volt}\\
    Current & \SI{45}{\micro \ampere}\\
    Gain & \SI{17}{\deci \bel}\\
    Exposure Time & \SI{1000}{\milli \second}\\
    Pixel Size & \SI{7.06}{\micro \meter} \\
    \hline
    \end{tabular}
\end{table}

\begin{table}[htbp]
    \centering
    \caption[Technical information of the IHLQ dataset]{Overview of technical information of the fast scan of the in-house dataset. This dataset is refered to as the \acrshort{ihlq} dataset. }
    \label{tab:inhouselq}
    \begin{tabular}{ll}
    \hline
    Instrument & Nikon XT H 225 ST \\
    Number of Projections & 150 \\
    Voltage & \SI{170}{\kilo \volt}\\
    Current & \SI{65}{\micro \ampere}\\
    Gain & \SI{24}{\deci \bel}\\
    Exposure Time & \SI{134}{\milli \second}\\
    Pixel Size & \SI{7.06}{\micro \meter} \\
    \hline
    \end{tabular}
\end{table}

\section{Compiling a Dataset for Training}
\label{sec:method:compilingdataset}
In this section, an explanation of how to compile a set of images into a suitable dataset for training the TomoGAN network, or to denoise with an already trained network, will be given. 

The process of preparing a dataset for either training or inferrence (i.e. denoising using an already trained network) consists of a few steps. A flowchart visualizing this process can be seen in \cref{fig:compilingdatasetflowchart}. The detailed steps are as follows:

\begin{enumerate}
    \item Begin with a full stack of reconstructed images. If the dataset is ment to be used to train the network, both a set of \acrfull{hq} reconstructions as well as a corresponding set (of the same dataset) of \acrfull{lq} reconstructions are needed. They must be sorted the same way so that image 0 of the \acrshort{hq} dataset corresponds to image 0 of the \acrshort{lq} dataset. If it is a datset that is to be denoised, only the \acrshort{lq} dataset is needed. 
    \item The images must then be cropped to remove unnecessary empty space and to help the network more easily focus on the important parts of the image. For a dataset that has not been properly cropped, the network may struggle to converge to a good solution, yielding poor results. This cropping step is done by visually inspecting the dataset and cropping an appropriate part of the images. If the dimension of the images is very large, it may be too large to properly fit in memory during training or denoising. The images must then be resized to an appropriate smaller size. 
    \item Once the images are cropped, they must be converted to the datatype uint8, as that is best suited for use in the network\footnote{Using uint8 instead of e.g. float32 reduces the memory footprint drastically, and the hardware used to perform the caluclations needed to train a neural network is often optimized for this type of calculations using uint8. }. This process consists of several steps. For each image in the dataset do the following:
    \begin{enumerate}
        \item First, find the minimum and maximum pixel values of an image. This can be the absolute minimum and maximum, but ideally it should be a lower and upper percentile of the pixel values to avoid singular extreme pixel values causing a loss of dynamic range in the image, as the pixel values will be limited to 256 distinct values. 
        \item Once the minimum and maximum values are determined, the image pixel values should be clipped to the previosly determined minimum and maximum values. This is necessary when the minimum and maximum values are percentiles to ensure all pixel values are within the new value range set by the percentiles.
        \item After clipping the pixel values, all pixel values can be scaled linearly between 0 and 255 according to the formula 
        \begin{equation}
            \label{eq:scaleimages}
            \hat{x} = 255 \cdot \frac{x - I_{min}}{I_{max} - I_{min}},
        \end{equation}
        where $\hat{x}$ is the updated scaled pixel value, $x$ is the old pixel value, and $I$ is the whole image.
    \end{enumerate}
    \item Once all images are scaled and converted to uint8, they can be filled into an HDF5-file containing one dataset for the \acrshort{hq} images, and if the dataset is to be used to train the network, one datset for the \acrshort{lq} images. The dimensions of the datasets should be $(N,H,W)$, where $N$ is the number of images, and $(H,W)$ are the dimensions of the images. 
    \item When the dataset is to be used to train the network, both the \acrshort{hq} and the \acrshort{lq} datasets must be split into two datasets: one for training, and one for testing during training. About $15\%$ of the total dataset should be set aside for testing. If the depth parameter of TomoGAN is to be used, both the training and test datasets need to be sorted (i.e. adjacent slices are adjacent in the dataset). 
\end{enumerate}


\begin{figure}[htbp]  
    \centering
    \includegraphics[width=.95\textwidth]{figures/compilingdatasetflowchart.pdf}
    \caption[Dataset creation flowchart]{Flowchart showing the process of compiling a dataset for training the TomoGAN denoising network. }
    \label{fig:compilingdatasetflowchart}
\end{figure}
