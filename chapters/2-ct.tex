\chapter{X-ray Computed Tomography}
\label{sec:ct}
While this thesis is primarily focused on noise reduction using \gls{gan}s, a brief overview of some of the basic principles behind \gls{ct} imaging will be given in this chapter. The focus will be on explaining the underlying theoretical foundation of \gls{ct} imaging, including the main sources of noise. An overview of common reconstruction methods, as well a more novel one, will also be presented.

\section{X-ray Attenuation in a Sample}
\label{sec:ct:theoreticalfoundation}
X-ray \gls{ct} imaging is based on interaction between X-rays and matter. This interaction will attenuate X-rays that propagate through a sample according to the Beer-Lambert law, which is given as \cite{doi:10.1063/1.4950807}
\begin{equation}
    \label{eq:beerlambert}
    I = I_0 \rm{e}^{-\int_{l_0}^{l}\mu\left(x,y,E\right)dl},
\end{equation}
where $I_0$ and $I$ are the incident and the attenuated X-ray beam intensities at positions $l_0$ and $l$ respectively, and $\mu$ is the attenuation coefficient of the traversed matter. The integral in the exponent is the path of the photon beam through the sample. The attenuation coefficient is dependent on energy ($E$), and typical X-ray \gls{ct} systems span a range of wavelengths (i.e. energies). 
Because of this, \cref{eq:beerlambert} must be modified to also account for the polychromatic nature of the X-ray source. 

The total incident radiation can be determined by integrating over all photon energies, 
\begin{equation}
    \label{eq:incidentradiation}
    I_0 = \int_{E_{min}}^{E_{max}}N\left(V,I\right)S\left(E\right)D\left(E\right)dE,
\end{equation}
with $N\left(V,I\right)$ being a variable introduced to account for photon flux depending on X-ray source tube voltage $V$ and current $I$, $S\left(E\right)$ being the normalized X-ray source spectrum modulated by the absorption materials between the source and the detector (not including the sample), and $D\left(E\right)$ being the detector sensitivity modulated by protection materials on the detector. $E_{min}$ and $E_{max}$ bound the energy range of the radiation spectrum. 

By combining \cref{eq:beerlambert,eq:incidentradiation}, we get the modified Beer-Lambert law accounting for the polychromatic X-rays \cite{doi:10.1063/1.4950807}, 
\begin{equation}
    I = \int_{E_{min}}^{E_{max}}N\left(V,I\right)S\left(E\right)D\left(E\right)\rm{e}^{-\int_{l_0}^{l}\mu\left(x,y,E\right)dl}dE.
\end{equation}
This can be solved for the attenuation coefficient projection, giving \cite{doi:10.1063/1.4950807}
\begin{equation}
    \label{eq:ctattenuationcoefficient}
    \int_{l_0}^{l} \mu_m\left(x,y\right)dl = -\ln\left[\frac{\int S\left(E\right) D\left(E\right)\rm{e}^{-\int_{l_0}^{l}\mu\left(x,y,E\right)dl} dE }{ \int S\left(E\right) D\left(E\right) dE }\right].
\end{equation}
The attenuated intensity $I$ can be related to the attenuation coefficient projection $\mu_m$ of a path through the sample by use of this equation. When the attenuation coefficient projections are known, the 2D attenuation coefficient map can be reconstructed using a reconstruction algorithm if a sufficient number of projections are available. Strategies for numerical \gls{ct} reconstructions will be described in \cref{sec:ct:reconstruction}.

\subsection{Noise}
\label{sec:ct:theory:noise}
Assuming a sufficient number of projections of the attenuation coefficient are available, the primary sources of noise in \gls{ct} measurements are quantum noise and electronic noise \cite{boas2012ct}. 

The quantum noise, sometimes also known as shot noise or simply Poisson noise, is due to the statistical error of low photon counts. It can be modeled as a Poisson distribution \cite{Whiting2006},
\begin{equation}
    \label{eq:poissonnoise}
    P(X = x) = \frac{\rm{e}^{-xm}m^x}{x!},
\end{equation}
with $m$ being the mean signal value, $x=0,1,...$ being an integer representing the measured signal value, and $X$ being a random variable denoting the number of photons generated by the X-ray source. Quantum noise can be reduced simply by increasing the incident X-ray beam intensity, however this is often not wanted as increasing the radiation dose has raised concerns about potential health risks \cite{doi:10.1056/NEJMra072149,PEARCE2012499}. 

Electronic noise is related to the electronics of the X-ray detector, and it is modeled as additive white Gaussian (i.e. normal) noise \cite{boas2012ct},
\begin{equation}
    \mathcal{N}\left(0,\sigma \right) = \frac{1}{2\pi\sigma}\rm{e}^{-\frac{x^2}{2\sigma}},
\end{equation}
which corresponds to a normal distribution with mean signal value of $0$ and standard deviation $\sigma$.

If an insufficient number of projections of the attenuation coefficient are available, it is known as a \textit{missing wedge problem}. Missing wedge measurements are incomplete datasets with respect to standard requirements of established reconstruction algorithms \cite{10.1111/jmi.12313}. This leads to artefacts that appear as elongations of reconstructed details along the mean direction (i.e. the symmetry centre of the projections). Several different reconstruction methods removing these artefacts have been attempted, also including \gls{ml} based approaches \cite{liu2020tomogan,GANrec}. Missing wedge artefacts will be refered to as noise in this thesis\todo[]{Not actually noise, change?}. 

A comparison of quantum noise and missing wedge noise on a dataset imaging glass beads is given in \cref{fig:noisecomparison}. 

\begin{figure}[htbp]  
    \centering
    \includegraphics[width=.9\textwidth]{figures/noisecomparison.pdf}
    \caption[Reconstruction noise comparison]{Comparison of a high-quality reconstruction, a quantum noise reconstruction, and a missing wedge reconstruction. Images d), e), and f) are zoomed in \gls{roi}s of images a), b), and c) respectively. The \gls{roi} is marked in a). The quantum noise is simulated by applying Poisson noise to the sinogram before reconstruction, and the missing wedge problem is simulated by selecting a subsampling of every 32nd projection from the high-quality reconstruction. The images are of a central slice from the dataset tomo\_00058 \cite{datasetglassspheres} reconstructed using \gls{fbp} from TomoPy \cite{TomoBank}. }
    \label{fig:noisecomparison}
\end{figure}

\section{Imaging Method}
\todo[inline]{Keep this section?}
\subsection{Setup}
\missingfigure{Figure of generic CT setup}
\subsection{Sinograms}
\missingfigure{Figure of image and its sinogram}

\section{CT Reconstruction}
\label{sec:ct:reconstruction}
After acquiring the attenuation coefficient projections, or sinograms\todo[]{Make figure showing an image and its sinogram. }, this data must be reconstructed into an image (of either 2D or 3D) of the object. The collection of a sinogram $P_\theta$ for projection angle $\theta$ is given by the Radon transform \cite{4307775,jimaging4110128}
\begin{equation}
    \label{eq:radontransform}
    P_\theta(u) = \iint f\left(x,y \right)\delta\left(x \cos \theta + y \sin \theta -u \right)dxdy,
\end{equation}
where $u$ is the position on the detector and $\delta$ is the Dirac delta function. In practice, because of computational and instrumental limitations, the projection data are acquired only for a limited number of projections $N_\theta$ as well as $N_d$ detector elements, and the imaged object is represented by a pixel grid of size $N \times N$. Thus, the acquired projection data is described by a vector $\bm{y} \in \mathbb{R}^{N_\theta \times N_d}$, the reconstructed image by a vector $\bm{x} \in \mathbb{R}^{N \times N}$, and the formation of the projection data can be stated as a linear system \cite{jimaging4110128}
\begin{equation}
    \label{eq:projectiondata}
    \bm{y} = \bm{A}\bm{x},
\end{equation}
where element $a_{ij} \in \mathbb{R}$ of $\bm{A} \in \mathbb{R}^{N_\theta N_d \times N^2}$ is equal to the contribution of image pixel $j$ to detector pixel $i$. This gives the tomographic reconstruction problem of recovering the unknown object $\bm{x}$ from the acquired projection data $\bm{y}$. It can be seen as performing the \textit{inverse} Radon transform \cite{KabanikhinIllVersed,GANrec}.\footnote{The inverse Radon transform is an \textit{inverse} problem. Inverse problems are often ill-posed. An ill-posed problem is a problem that does not meet any one or more of the three conditions suggested by Jacques Hadamard: existence, uniqueness, and stability \cite{KabanikhinIllVersed}. The stability condition is most often violated. This means that its output is highly sensitive to small changes in the input (e.g. noise can drastically change the resulting output). }

Conventional tomographic reconstruction algorithms are generally divided into two groups: direct reconstruction, and iterative reconstruction, however a third method using \gls{ml} has also shown promise \cite{GANrec}.

\subsection{Direct Reconstruction}
Direct tomographic reconstruction algorithms are based on finding an inversion formula of the continuous forward Radon transform, as given in \cref{eq:radontransform}. The numerical implementation is done by using a discretized inverse Radon transform \cite{jimaging4110128}. The most commonly used direct reconstruction algorithm is the \gls{fbp} algorithm, which can be written as \cite{jimaging4110128}
\begin{equation}
    \label{eq:fbp}
    \bm{x}_{FBP} = \bm{A}^T \bm{C}_h \bm{y},
\end{equation}
where $\bm{C}_h$ is a 1D convolution operation that convolves each detector row in $\bm{y}$ with a filter $\bm{h} \in \mathbb{R}^{N_d}$. The filter is typically some standard filter that can be used for any reconstruction (e.g. the Ram-Lak filter), and may include low-pass filtering to reduce high frequency noise in the reconstructed image \cite{681991}. It has also been shown that this filter can be learned by use of \gls{ml} (more specifically an \gls{ann}) to further improve the performance of \gls{fbp} \cite{6607157}. 
\missingfigure{Illustration of FBP with projections leading to subsampling in Fourier space}
Direct algorithms, such as \gls{fbp}, have the advantage of most often being computationally efficient, as well as producing accurate results when enough projections are available \cite{jimaging4110128}. It can be shown that the sufficient number of projections in \gls{fbp} is ...\todo[]{Write this} The issue with these techniques arises when only a limited number of projections are available, as they are generally highly prone to noise leading to insufficient image quality for further analysis \cite{jimaging4110128}. This is where the use for iterative reconstruction algorithms arises.

For \gls{ct} imaging systems where the X-ray beam is conical (as opposed to parallel), an alternative direct reconstruction method similar to \gls{fbp} is the \gls{fdk} reconstruction algorithm \cite{Feldkamp:84}. 

\subsection{Iterative Reconstruction}
\label{sec:ct:reconstruction:iterative}
Iterative tomographic reconstruction algorithms are based on iteratively solving the linear system given in \cref{eq:projectiondata}. Iterative \gls{ct} algorithms can provide reconstructions with fewer artefacts and less noise than the \gls{fbp} algorithm, in particular when using few measured projections. A common method used is to find images that minimize the $l^2$-norm of the residual error (i.e. the difference between the acquired sinograms, and the Radon transform of the reconstructed image\footnote{Calculating the inverse Radon transform is an ill-posed problem and is challenging, however calculating the Radon transform itself is an easy task. }) as well as an additional term $g$ that penalizes images that do not confine to some prior knowledge or assumption of the imaged object. This process can be written as \cite{jimaging4110128}
\begin{equation}
    \label{eq:iterativesolution}
    \bm{x}_{iter} = \underset{\bm{x}}{\text{argmin}} \left|\left|\bm{y} - \bm{A}\bm{x}\right|\right|_2^2 + \lambda g(\bm{x}),
\end{equation}
where $\left|\left| \bm{\cdot} \right|\right|_2^2$ denotes the $l^2$-norm, and $\lambda$ is the relative weight of the prior knowledge penalty compared to the residual error. If a prior knowledge penalty that fits a reconstruction well is chosen, iterative reconstruction algorithms can produce significantly more accurate reconstructions than direct methods when reconstructing from limited data \cite{jimaging4110128}. If the chosen prior knowledge penalty does not fit well however, or if the weighting parameter $\lambda$ is poorly selected as it is a problem-dependent parameter, it may lead to poor quality reconstructions. 

A large drawback with iterative reconstruction is its (often) large computational cost. These types of reconstructions are slower, which may make it difficult to apply them to time-sensitive real-world tomographic data \cite{jimaging4110128}. Newer and more powerful computers can to some extent offset this downside to iterative reconstructions \cite{willemink2013iterative}. 

Because of these limitations and drawbacks, direct reconstruction algorithms are still often preferred in many fields \cite{Pan_2009}. 

\todo[inline]{This next part needs fixing}
Another iterative reconstruction algorithm is known as \gls{piccs} \cite{piccs}. It is used to better reconstruct datasets that are limited in the number of projections available when multiple imagings have been done for several time frames (known as dynamic \gls{ct} imaging). By reconstructing a prior image from the union of interleaved datasets from several time frames, the \gls{piccs} reconstruction algorithm utilizes the spatial-temporal correlations in the imaging to make assumptions on the imaged object. The prior image is used as a constrain on the reconstruction of the missing wedge prone reconstructions of each time frame. 


\subsection{Other Methods}
\todo[inline]{Change structure, is this section needed?}
In addition to direct and iterative reconstruction techniques, \gls{ml} has been used to make an iterative-like reconstruction algorithm \cite{GANrec}. One method, termed GANrec, is based on a \gls{gan} (see \cref{sec:ml:types:gan}) and is an \gls{ml} method that does not require training of the network before reconstruction, instead using the training process as the reconstruction process. 

It takes a given sinogram $\bm{y}$ and uses the generating network $G$ to create a candidate reconstructed image $\bm{x} = G(\bm{y})$, then creates the corresponding candidate sinogram $\hat{\bm{y}} = P(\bm{x})$, where $P$ is the Radon transform. The loss $L = \left|\left| y - \hat{y} \right|\right|$ is then the basis of training the network (i.e. reconstructing the image).\footnote{The actual loss used for training GANrec also includes an adversarial loss, which will be introduced in \cref{sec:ml:training:lossfunctions}, however it is omitted in this description for the sake of simplicity. }

The sinogram-to-reconstruction transformation cannot be done by a conventional \gls{cnn}-style network, however it has been shown that a single fully connected layer can perform this transformation \cite{PASCHALIS2004211}. The accuracy of the transformation can be improved by increasing the number of layers and neurons, however this is dependent on the available computational power \cite{GANrec}. Because of this limitation, the generating network in GANrec is a modified version of U-net \cite{unet} (see \cref{sec:ml:types:encoderdecoder}), with three fully connected layers at the start to perform this transform. 